{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "42modelos_matrices.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGwWvOXRks6k",
        "outputId": "8edaba9a-c198-4414-b533-45e33f3fa1d1"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive/')\n",
        "\n",
        "%cd /content/gdrive/My\\ Drive/TESIS/autism-master/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n",
            "/content/gdrive/My Drive/TESIS/autism-master\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqESixZxkuXc"
      },
      "source": [
        "%%capture\n",
        "import os\n",
        "import sys\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import cross_validate\n",
        "!{sys.executable} -m pip install https://api.github.com/repos/paris-saclay-cds/ramp-workflow/zipball/master\n",
        "!{sys.executable} -m pip install scikit-learn seaborn nilearn\n",
        "from problem import get_cv\n",
        "from download_data import fetch_fmri_time_series\n",
        "from problem import get_train_data\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhnIrO_hk8zS"
      },
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "from nilearn.connectome import ConnectivityMeasure\n",
        "\n",
        "def _load_fmri(fmri_filenames):\n",
        "    \"\"\"Load time-series extracted from the fMRI using a specific atlas.\"\"\"\n",
        "    return np.array([pd.read_csv(subject_filename,\n",
        "                                 header=None).values\n",
        "                     for subject_filename in fmri_filenames])\n",
        "\n",
        "class FeatureExtractor(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def __init__(self, kind = 'correlation', atlas= 'basc064'):\n",
        "        # make a transformer which will load the time series and compute the\n",
        "        # connectome matrix\n",
        "        self.kindCM = kind\n",
        "        self.atlas = 'fmri_'+atlas\n",
        "        self.transformer_fmri = make_pipeline(\n",
        "            FunctionTransformer(func=_load_fmri, validate=False),\n",
        "            ConnectivityMeasure(kind=self.kindCM, vectorize=False))\n",
        "\n",
        "    def fit(self, X_df, y):\n",
        "        fmri_filenames = X_df[self.atlas]\n",
        "        self.transformer_fmri.fit(fmri_filenames, y)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X_df):\n",
        "        fmri_filenames = X_df[self.atlas]\n",
        "        X_con = self.transformer_fmri.transform(fmri_filenames)\n",
        "        X_con = pd.DataFrame(X_con)\n",
        "        X_con = X_df.index\n",
        "        X_con.columns = ['fmri_{}'.format(i) for i in range(X_con.columns.size)]\n",
        "        print('ended transform')                        \n",
        "        # get the anatomical information\n",
        "        X_anatomy = X_df[[col for col in X_df.columns\n",
        "                          if col.startswith('anatomy')]]\n",
        "        X_anatomy = X_anatomy.drop(columns='anatomy_select')\n",
        "        cols = X_anatomy.columns\n",
        "        scaler = StandardScaler()\n",
        "        X_anatomy = scaler.fit_transform(X_anatomy)\n",
        "        X_anatomy = pd.DataFrame(X_anatomy, index=X_df.index)\n",
        "        X_anatomy.columns = cols\n",
        "        # concatenate both matrices\n",
        "        return pd.concat([X_anatomy, X_con], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sd5VYQ0RmSW6"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "class Classifier(BaseEstimator):\n",
        "    def __init__(self, est = 'RF'):\n",
        "      if est == 'RF':\n",
        "        self.clf = RandomForestClassifier(n_estimators = 300)\n",
        "      elif est == 'XGB': \n",
        "        self.clf = XGBClassifier()\n",
        "      self.clf_ripser = make_pipeline(StandardScaler(), self.clf)\n",
        "    def fit(self, X, y):\n",
        "        self.clf_ripser.fit(X, y)\n",
        "        return self\n",
        "    def predict(self, X):\n",
        "        return self.clf_ripser.predict(X)\n",
        "    def predict_proba(self, X):\n",
        "        return self.clf_ripser.predict_proba(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUNPLiCHk8gh"
      },
      "source": [
        "data_train, labels_train = get_train_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xcfyNWLkzV7"
      },
      "source": [
        "def evaluation(X, y, ext = FeatureExtractor(), cla = Classifier()):\n",
        "    pipe = make_pipeline(ext, cla)\n",
        "    cv = get_cv(X, y)\n",
        "    results = cross_validate(pipe, X, y, scoring=['roc_auc', 'accuracy', 'precision', 'recall'], cv=cv,\n",
        "                             verbose=1, return_train_score=True,\n",
        "                             n_jobs=-1)\n",
        "    \n",
        "    return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WLAis7snPkF"
      },
      "source": [
        "atlass =  ['power_2011'] #'',['basc122', 'basc197', 'craddock_scorr_mean', 'power_2011','msdl', 'basc064', 'harvard_oxford_cort_prob_2mm']\n",
        "mattype = ['tangent', 'correlation']#['partial correlation', 'tangent', 'correlation']\n",
        "classif = ['XGB', 'RF']\n",
        "\n",
        "for a in [1]:\n",
        "  for m in [1]:\n",
        "    for case in remaining:\n",
        "      atl, mat, clas = case\n",
        "      class FeatureExtractor(BaseEstimator, TransformerMixin):\n",
        "          def __init__(self):\n",
        "              # make a transformer which will load the time series and compute the\n",
        "              # connectome matrix\n",
        "              self.kindCM = mat\n",
        "              self.atlas = 'fmri_'+atl\n",
        "              self.transformer_fmri = make_pipeline(\n",
        "                  FunctionTransformer(func=_load_fmri, validate=False),\n",
        "                  ConnectivityMeasure(kind=self.kindCM, vectorize=True))\n",
        "          def fit(self, X_df, y):\n",
        "              fmri_filenames = X_df[self.atlas]\n",
        "              self.transformer_fmri.fit(fmri_filenames, y)\n",
        "              return self\n",
        "          def transform(self, X_df):\n",
        "              fmri_filenames = X_df[self.atlas]\n",
        "              X_con = self.transformer_fmri.transform(fmri_filenames)\n",
        "              X_con = pd.DataFrame(X_con)\n",
        "              X_con.index = X_df.index\n",
        "              X_con.columns = ['fmri_{}'.format(i) for i in range(X_con.columns.size)]\n",
        "              print('ended transform')                        \n",
        "              # get the anatomical information\n",
        "              X_anatomy = X_df[[col for col in X_df.columns if col.startswith('anatomy')]]\n",
        "              X_anatomy = X_anatomy.drop(columns='anatomy_select')\n",
        "              cols = X_anatomy.columns\n",
        "              scaler = StandardScaler()\n",
        "              X_anatomy = scaler.fit_transform(X_anatomy)\n",
        "              X_anatomy = pd.DataFrame(X_anatomy, index=X_df.index)\n",
        "              X_anatomy.columns = cols\n",
        "              # concatenate both matrices\n",
        "              return pd.concat([X_anatomy, X_con], axis=1)\n",
        "      \n",
        "      class Classifier(BaseEstimator):\n",
        "          def __init__(self):\n",
        "            self.est = clas\n",
        "            if self.est == 'RF':\n",
        "              self.clf = RandomForestClassifier(n_estimators = 300)\n",
        "            elif self.est == 'XGB': \n",
        "              self.clf = XGBClassifier()\n",
        "            self.clf_ripser = make_pipeline(StandardScaler(), self.clf)\n",
        "          def fit(self, X, y):\n",
        "              self.clf_ripser.fit(X, y)\n",
        "              return self\n",
        "          def predict(self, X):\n",
        "              return self.clf_ripser.predict(X)\n",
        "          def predict_proba(self, X):\n",
        "              return self.clf_ripser.predict_proba(X)\n",
        "\n",
        "      results = evaluation(data_train, labels_train, ext= FeatureExtractor(), cla=Classifier())\n",
        "      print(f'{atl},{mat},{clas}')\n",
        "      print(\"ROC-AUC: {:.3f} +- {:.3f}\".format(np.mean(results['test_roc_auc']), np.std(results['test_roc_auc'])))\n",
        "      print(\"accuracy: {:.3f} +- {:.3f}\".format(np.mean(results['test_accuracy']), np.std(results['test_accuracy'])))\n",
        "      print(\"precision: {:.3f} +- {:.3f}\".format(np.mean(results['test_precision']), np.std(results['test_precision'])))\n",
        "      print(\"recall: {:.3f} +- {:.3f}\".format(np.mean(results['test_recall']), np.std(results['test_recall'])))\n",
        "\n",
        "# MATRIZ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPHR8WBjfPoz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZN5qFzn5M7N"
      },
      "source": [
        "%%capture\n",
        "import os\n",
        "import sys\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import cross_validate\n",
        "!{sys.executable} -m pip install https://api.github.com/repos/paris-saclay-cds/ramp-workflow/zipball/master\n",
        "!{sys.executable} -m pip install scikit-learn seaborn nilearn\n",
        "from problem import get_cv\n",
        "from download_data import fetch_fmri_time_series\n",
        "from problem import get_train_data\n",
        "!pip install ripser\n",
        "from ripser import ripser\n",
        "from persim import plot_diagrams\n",
        "!pip install tensorflow_addons  \n",
        "!pip install gudhi\n",
        "import pickle as pkl\n",
        "from sklearn.ensemble import *\n",
        "from sklearn.svm import *\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import GridSearchCV, KFold, ShuffleSplit\n",
        "from tensorflow import random_uniform_initializer as rui\n",
        "import gudhi.representations as tda\n",
        "import tensorflow as tf\n",
        "import os.path\n",
        "import itertools\n",
        "import h5py\n",
        "import tensorflow_addons  as tfa\n",
        "import gudhi              as gd\n",
        "from scipy.sparse           import csgraph\n",
        "from scipy.io               import loadmat\n",
        "from scipy.linalg           import eigh\n",
        "from sklearn.preprocessing  import LabelEncoder, OneHotEncoder\n",
        "from sklearn.cluster import KMeans\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "# Reduccion dimensionalidad\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "!pip install git+https://github.com/MathieuCarriere/perslay\n",
        "!pip install git+https://github.com/MathieuCarriere/sklearn-tda\n",
        "from perslay import PerslayModel\n",
        "from scipy.stats import beta\n",
        "!pip install nilearn\n",
        "from nilearn.connectome import ConnectivityMeasure\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install xgboost\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning) \n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "from nilearn.connectome import ConnectivityMeasure\n",
        "from nilearn.connectome import vec_to_sym_matrix\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from perslay import PerslayModel\n",
        "from ripser import ripser\n",
        "import tensorflow_addons  as tfa\n",
        "import gudhi.representations as tda\n",
        "import tensorflow as tf\n",
        "import gudhi              as gd\n",
        "from tensorflow import random_uniform_initializer as rui\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "\n",
        "def ripser_diagram(paciente, hom_over_p=2, hom_dimension = 1, distm = True):\n",
        "    diag = ripser(paciente, distance_matrix=distm, coeff = hom_over_p, maxdim=hom_dimension)['dgms']\n",
        "    return(diag)\n",
        "\n",
        "def gen_diags_dict(Dg):\n",
        "  Len = len(Dg)\n",
        "  diags_dict = {'Rips_dim_0': [],'Rips_dim_1': []}\n",
        "  ep = 0.01\n",
        "  for i in range(Len):\n",
        "    if len(Dg[i][0]) == 0:\n",
        "      diags_dict['Rips_dim_0'].append(np.array([[1,1+ep],[1.5,1.5+ep], [0.5,0.5+ep]]))\n",
        "    else:\n",
        "      diags_dict['Rips_dim_0'].append(np.array(Dg[i][0]))\n",
        "    if len(Dg[i][1]) == 0:\n",
        "      diags_dict['Rips_dim_1'].append(np.array([[1,1+ep],[1.5,1.5+ep], [0.5,0.5+ep]]))\n",
        "    else:\n",
        "      diags_dict['Rips_dim_1'].append(np.array(Dg[i][1]))\n",
        "  return diags_dict\n",
        "\n",
        "def diag_prepro(diags_dict, use_selector = True, use_prominent = True, thresh = 500, use_scaler = True, use_padding = True):\n",
        "  \n",
        "  # https://gudhi.inria.fr/python/3.1.0.rc1/representations.html\n",
        "  # Whole pipeline\n",
        "  tmp = Pipeline([\n",
        "          (\"Selector\",      tda.DiagramSelector(use=use_selector, point_type=\"finite\")),\n",
        "          (\"ProminentPts\",  tda.ProminentPoints(use=use_prominent, num_pts=thresh)),\n",
        "          (\"Scaler\",        tda.DiagramScaler(use=use_scaler, scalers=[([0,1], MinMaxScaler())])),\n",
        "          (\"Padding\",       tda.Padding(use=use_padding)),\n",
        "                  ])\n",
        "\n",
        "  prm = {filt: {\"ProminentPts__num_pts\": min(thresh, max([len(dgm) for dgm in diags_dict[filt]]))} \n",
        "        for filt in diags_dict.keys() if max([len(dgm) for dgm in diags_dict[filt]]) > 0}\n",
        "\n",
        "  # Apply the previous pipeline on the different filtrations.\n",
        "  diags = []\n",
        "  for dt in prm.keys():\n",
        "      param = prm[dt]\n",
        "      tmp.set_params(**param)\n",
        "      diags.append(tmp.fit_transform(diags_dict[dt]))\n",
        "\n",
        "  # For each filtration, concatenate all diagrams in a single array.\n",
        "  D, npts = [], len(diags[0])\n",
        "  for dt in range(len(prm.keys())):\n",
        "      D.append(np.array(np.concatenate([diags[dt][i][np.newaxis,:] for i in range(npts)],axis=0),dtype=np.float32))\n",
        "  \n",
        "  return D\n",
        "\n",
        "def load_architecture_params(weight, len_d, layer = \"Image\", perm_op = \"mean\"):\n",
        "  perslay_parameters = []\n",
        "  perslay_channel = {}\n",
        "\n",
        "  perslay_channel[\"pweight_train\"] = True\n",
        "  perslay_channel[\"layer_train\"]   = True\n",
        "  perslay_channel[\"final_model\"]   = tf.keras.Sequential([tf.keras.layers.Flatten()])\n",
        "\n",
        "  if layer == \"Landscape\":\n",
        "    perslay_channel[\"layer\"]           = \"Landscape\"\n",
        "    perslay_channel[\"lsample_num\"]     = 100\n",
        "    perslay_channel[\"lsample_init\"]    = rui(0.0, 1.0)\n",
        "\n",
        "  if layer == \"Image\":\n",
        "    perslay_channel[\"layer\"]           = \"Image\"\n",
        "    perslay_channel[\"image_size\"]      = (20, 20)\n",
        "    perslay_channel[\"image_bnds\"]      = ((-.001, 2.001), (-.001, 2.001))\n",
        "    perslay_channel[\"lvariance_init\"]  = 3.\n",
        "\n",
        "\n",
        "  if weight == \"gmix\":\n",
        "    perslay_channel[\"pweight\"]       = \"gmix\"\n",
        "    perslay_channel[\"pweight_num\"]   = 3\n",
        "    perslay_channel[\"pweight_init\"]  = np.array(np.vstack([np.random.uniform(0.,1.,[2,3]),\n",
        "                                                          5.*np.ones([2,3])]), dtype=np.float32)\n",
        "  if weight == \"power\":\n",
        "    perslay_channel[\"pweight\"]       = \"power\"\n",
        "    perslay_channel[\"pweight_init\"]  = 1.\n",
        "    perslay_channel[\"pweight_power\"] = 1\n",
        "\n",
        "  if weight == \"grid\":\n",
        "    perslay_channel[\"pweight\"]       = \"grid\"\n",
        "    perslay_channel[\"pweight_size\"]  = [20,20]\n",
        "    perslay_channel[\"pweight_bnds\"]  = ((-.001, 1.001), (-.001, 1.001))\n",
        "    perslay_channel[\"pweight_init\"]  = rui(1.0, 1.0)\n",
        "  \n",
        "  if weight == \"None\":\n",
        "    perslay_channel[\"pweight\"]       = None\n",
        "\n",
        "  perslay_channel[\"perm_op\"] = perm_op\n",
        "  # mean, max, sum, topk\n",
        "\n",
        "  if perm_op == \"topk\":\n",
        "    perslay_channel[\"keep\"]  = 10\n",
        "\n",
        "  perslay_parameters = [perslay_channel for _ in range(len_d)]\n",
        "\n",
        "  return perslay_parameters\n",
        "\n",
        "def vector_representations(D, perslay_parameters):\n",
        "  mirrored_strategy = tf.distribute.MirroredStrategy()\n",
        "  with mirrored_strategy.scope():\n",
        "      \n",
        "      # Final rho network is a simple dense layer to the number of labels \n",
        "      rho = tf.keras.Sequential([tf.keras.layers.Dense(2, activation=\"sigmoid\", input_shape=(800,))])\n",
        "      model = PerslayModel(name=\"PersLay\", diagdim=2, perslay_parameters=perslay_parameters, rho=rho)\n",
        "\n",
        "      # Optimizer is Adam with exponential decay of learning rate and moving average of variables\n",
        "      lr = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=.01, decay_steps=20, decay_rate=0.5)\n",
        "      optimizer = tf.keras.optimizers.Adam(learning_rate=lr, epsilon=1e-4)\n",
        "      optimizer = tfa.optimizers.MovingAverage(optimizer, average_decay=0.9) \n",
        "\n",
        "      # Loss is cross-entropy\n",
        "      loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "      # Metric is accuracy\n",
        "      metrics = [tf.keras.metrics.CategoricalAccuracy()]\n",
        "\n",
        "  vectors = model.compute_representations(D).numpy()\n",
        "  return vectors\n",
        "\n",
        "def triag_array(array):\n",
        "    return(array[np.triu_indices_from(array)])\n",
        "\n",
        "def gen_ripser_from_connectome(X_connectome, name = 'ripser'):\n",
        "    X_rips = []\n",
        "    Len = len(X_connectome)\n",
        "    for i in range(Len):\n",
        "        dgm = ripser_diagram(X_connectome[i])\n",
        "        X_rips.append(dgm)\n",
        "    X_rips = np.nan_to_num(X_rips)\n",
        "    \t\t# get the ripser diagram\n",
        "    diags_dict = gen_diags_dict(X_rips)\n",
        "    F = np.array([[]]*Len)\n",
        "    D = diag_prepro(diags_dict, use_selector = True, use_prominent = True, thresh = 40, use_scaler = False, use_padding = True)\n",
        "    lend = len(D)\n",
        "    perslay_parameters = load_architecture_params(weight=\"None\", len_d = lend, layer= \"Image\", perm_op = \"sum\")\n",
        "    X_ripser = vector_representations(D, perslay_parameters)\n",
        "    X_ripser = np.nan_to_num(X_ripser)\n",
        "    X_ripser = pd.DataFrame(X_ripser)\n",
        "    return X_ripser\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SJLV8mJfXt2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26171415-90ce-4ab9-eac8-6cbc89147a5b"
      },
      "source": [
        "atlass =  [1] #['basc122', 'basc197', 'craddock_scorr_mean', 'power_2011','msdl', 'basc064', 'harvard_oxford_cort_prob_2mm']\n",
        "mattype = [1]#['partial correlation', 'tangent', 'correlation']\n",
        "#classif = ['XGB', 'RF']\n",
        "\n",
        "#últimos tres acá\n",
        "remaining = [['basc197',\t 'partial correlation',\t 'XGB'],\n",
        "             ['basc197',\t 'partial correlation',\t 'RF'],\n",
        "             ['basc197',\t 'tangent',\t 'XGB']]\n",
        "\n",
        "for A in atlass:\n",
        "  for B in mattype:\n",
        "    for case in remaining:\n",
        "      atl, mat, clas = case[0], case[1], case[2]\n",
        "      print(case)\n",
        "      class FeatureExtractor(BaseEstimator, TransformerMixin):\n",
        "\n",
        "          def __init__(self):\n",
        "              # make a transformer which will load the time series and compute the\n",
        "              # connectome matrix\n",
        "              self.kindCM = mat\n",
        "              self.atlas = 'fmri_'+ atl\n",
        "              self.transformer_fmri = make_pipeline(\n",
        "                  FunctionTransformer(func=_load_fmri, validate=False),\n",
        "                  ConnectivityMeasure(kind=self.kindCM, vectorize=False))\n",
        "\n",
        "          def fit(self, X_df, y):\n",
        "              fmri_filenames = X_df[self.atlas]\n",
        "              self.transformer_fmri.fit(fmri_filenames, y)\n",
        "              return self\n",
        "\n",
        "          def transform(self, X_df):\n",
        "              fmri_filenames = X_df[self.atlas]\n",
        "              X_con = self.transformer_fmri.transform(fmri_filenames)\n",
        "              max_value = np.amax(np.abs(X_con)) + 0.0001\n",
        "              for i in range(X_con.shape[0]):\n",
        "                  X_con[i] = np.sqrt(2*(1-X_con[i]/max_value))\n",
        "              # get the ripser diagram\n",
        "              print('began ripser transform')\n",
        "              X_ripser = gen_ripser_from_connectome(X_con, name = 'ripser')\n",
        "              X_ripser.index = X_df.index\n",
        "              X_ripser.columns = ['ripser_{}'.format(i)\n",
        "                                      for i in range(X_ripser.columns.size)]\n",
        "              print('ended ripser transform')                        \n",
        "              # get the anatomical information\n",
        "              X_anatomy = X_df[[col for col in X_df.columns\n",
        "                                if col.startswith('anatomy')]]\n",
        "              X_anatomy = X_anatomy.drop(columns='anatomy_select')\n",
        "              cols = X_anatomy.columns\n",
        "              scaler = StandardScaler()\n",
        "              X_anatomy = scaler.fit_transform(X_anatomy)\n",
        "              X_anatomy = pd.DataFrame(X_anatomy, index=X_df.index)\n",
        "              X_anatomy.columns = cols\n",
        "              \n",
        "              # concatenate both matrices\n",
        "              return pd.concat([X_anatomy, X_ripser], axis=1)\n",
        "      \n",
        "      class Classifier(BaseEstimator):\n",
        "          def __init__(self):\n",
        "            self.est = clas\n",
        "            if self.est == 'RF':\n",
        "              self.clf = RandomForestClassifier(n_estimators = 300)\n",
        "            elif self.est == 'XGB': \n",
        "              self.clf = XGBClassifier()\n",
        "            self.clf_ripser = make_pipeline(StandardScaler(), self.clf)\n",
        "          def fit(self, X, y):\n",
        "              self.clf_ripser.fit(X, y)\n",
        "              return self\n",
        "          def predict(self, X):\n",
        "              return self.clf_ripser.predict(X)\n",
        "          def predict_proba(self, X):\n",
        "              return self.clf_ripser.predict_proba(X)\n",
        "\n",
        "      results = evaluation(data_train, labels_train, ext= FeatureExtractor(), cla=Classifier())\n",
        "      print(f'{atl},{mat},{clas}')\n",
        "      print(\"ROC-AUC: {:.3f} +- {:.3f}\".format(np.mean(results['test_roc_auc']), np.std(results['test_roc_auc'])))\n",
        "      print(\"accuracy: {:.3f} +- {:.3f}\".format(np.mean(results['test_accuracy']), np.std(results['test_accuracy'])))\n",
        "      print(\"precision: {:.3f} +- {:.3f}\".format(np.mean(results['test_precision']), np.std(results['test_precision'])))\n",
        "      print(\"recall: {:.3f} +- {:.3f}\".format(np.mean(results['test_recall']), np.std(results['test_recall'])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "basc197,tangent,XGB\n",
            "ROC-AUC: 0.632 +- 0.033\n",
            "accuracy: 0.585 +- 0.031\n",
            "precision: 0.560 +- 0.028\n",
            "recall: 0.617 +- 0.049\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed: 71.0min finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFRrdZlhtYjk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}